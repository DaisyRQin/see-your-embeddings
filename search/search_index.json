{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Dimensionality reduction for cohort data visualization is to plot a high-dimensional dataset in 2,3\\text{-D} spaces to help analysts to identify data properties and gain insights visually. To maximize the information conveyed by plots, three aspects of local neighborhood preservation, cohort structure preservation, and cohort separation are required to be satisfied. COVA and ANGEL are novel data visualization algorithms providing a simple but effective way to comprehensively improve cohort data visualization performance. The core strategy of the algorithms are the following: Applying prototypes/anchor points to represent the global structure of the dataset. Generating data points around the prototypes/anchor points to maintain the local and global structural preservation. Hyperparameters can be adjusts to enhance the cohort separation. A set of new evaluation approaches are also proposed to assess preservation quality fairly. Experimental results show that the proposed algorithm can provide informative visualization results. The details for the mathematics of the algorithms can be found in our papers: COVA . The main layout of this documentation website is as the following: Tutorial: How COVA and ANGEL algorithms works COVA Tutorial ANGEL Tutorial Evaluation Demo : Demos for helping to understand algorithms Key Knowledge : Explaining key terminologies and concepts adjacency matrix cohort distance cohort membership confidence matrix isometric embedding kmeans clustering Local Anchor Embedding Soft Ordinal Embedding Function Lists : API guide ANGEL AnchorPointGeneration AnchorEmbedding ANGEL_embedding COVA PrototypeGeneration PrototypeEmbedding CohortConfidence COVAembedding Evaluation Prev FunctionFile: loadData DataScaler CohortDistance AdjacencyMatrix localanchorembedding AnchorGraph SOEembedding SOE Examples : Examples results of applying algorithms on real-world datasets","title":"Home"},{"location":"#introduction","text":"Dimensionality reduction for cohort data visualization is to plot a high-dimensional dataset in 2,3\\text{-D} spaces to help analysts to identify data properties and gain insights visually. To maximize the information conveyed by plots, three aspects of local neighborhood preservation, cohort structure preservation, and cohort separation are required to be satisfied. COVA and ANGEL are novel data visualization algorithms providing a simple but effective way to comprehensively improve cohort data visualization performance. The core strategy of the algorithms are the following: Applying prototypes/anchor points to represent the global structure of the dataset. Generating data points around the prototypes/anchor points to maintain the local and global structural preservation. Hyperparameters can be adjusts to enhance the cohort separation. A set of new evaluation approaches are also proposed to assess preservation quality fairly. Experimental results show that the proposed algorithm can provide informative visualization results. The details for the mathematics of the algorithms can be found in our papers: COVA . The main layout of this documentation website is as the following: Tutorial: How COVA and ANGEL algorithms works COVA Tutorial ANGEL Tutorial Evaluation Demo : Demos for helping to understand algorithms Key Knowledge : Explaining key terminologies and concepts adjacency matrix cohort distance cohort membership confidence matrix isometric embedding kmeans clustering Local Anchor Embedding Soft Ordinal Embedding Function Lists : API guide ANGEL AnchorPointGeneration AnchorEmbedding ANGEL_embedding COVA PrototypeGeneration PrototypeEmbedding CohortConfidence COVAembedding Evaluation Prev FunctionFile: loadData DataScaler CohortDistance AdjacencyMatrix localanchorembedding AnchorGraph SOEembedding SOE Examples : Examples results of applying algorithms on real-world datasets","title":"Introduction"},{"location":"COVATutorial/","text":"COVA Tutorial We are going to show how COVA works by embedding an example set of 2-dimensional data points in a new 2D space. The goal is to preserve as well as possible the cohort structure of the data and the local neighbourhood relation between the individual data points. How COVA Works First, we load the data. The example dataset contains n=1000 samples of d=2 dimensions belonging to c=7 cohorts. The coordinates of the data samples are stored in the matrix Data , while the cohort membership of each data sample is stored in DataCohort . DataInput, DataCohort = loadData('OneFlower.mat') def loadData(datapath): # The function will load the data from the local disk # :param datapath: could be direct path or indirect path, right now we only support '.mat' format. After loading the data, we preprocess the dataset by applying a certain scaler to the dataset. Here, we utilize the MinMax scaler to transform features by scaling each feature to a given range. Data = DataScaler(DataInput) def DataScaler(DataInput): # The function scales the dataset into a given range, here we fix the range to be [0,1] # :param DataInput: input data matrix in the high-dimensional space, D times n with D: feature dimension, n: number of data samples # :return: # Data: the scaled data matrix in the high-dimensional space, D times n with D: feature dimension, n: number of data samples Next, we estimate the Cohort Distance between any two data cohorts, and store the estimated distances in the matrix D_c . There are two different ways to calculate the cohort distance, which is selected by changing the hyper-parameter typeCohort . When typeCohort=0 , we keep the original data cohorts, and apply the cohort distance calculation. When typeCohort=1 , we divide each data cohorts into several sub-cohorts using K-means clustering algorithm , and calculate the cohort distance based on new generated cohorts. Dc, newlabel = ProtoGeneration(Data, DataCohort, typeCohort=0) def ProtoGeneration(Data, DataCohort, typeCohort=0): # Cohort distance calculation. # The default metric is based on the Euclidean distance. # # :param Data: input data matrix in the high-dimensional space, D times n with D: feature dimension, n: number of data samples # :param DataCohort: input label column # :param typeCohort: whether to use the original data cohorts or generating new sub-cohorts. 0: original; 1: new sub-cohorts # # :return: # Dc: the matrix with each indices being the distance between two cohorts # newlabel: keep the same to DataCohort when typeCohort=0, return the new generated data labels when typeCohort=1. Taking D_c as the input, we use an isometry embedding to generate cohort prototypes (one for each cohort), so that the Euclidean distances between two prototypes preserve the between-cohort distance values in D_c . Similarly, when typeCohort=0 , we will obtain c=7 prototypes which is identical to the number of original data cohort; when typeCohort=1 , the obtained prototypes are more than 7 , and here we fixing the c = \\lfloor0.1n\\rfloor . The computed prototype embeddings are stored in the matrix V . Here, the motivation of applying an isometric embedding algorithm for cohorts rather than individual points is that an algorithm of this type, e.g., multi-dimensional embedding and ordinal embedding, has better chance to succeed when handling a small number of instances. V0 = ProtoEmbedding(Dc, DataCohort) def ProtoEmbedding(Data, DataCohort, embedding = 'SOE'): # Prototypes embedding. # Finding the representative points of prototypes on 2d/3d spaces # # :param Dc: the matrix with each indices being the distance between two cohorts # :param DataCohort: input label column # # :return: # V: the embedded prototype points on the low-dimensional space We also scale our embedded prototypes to a certain range by using the DataScaler V= DataScaler(V0) Now, we look at the neighbouring relations between individual points. We compute an adjacency matrix W between the n=1000 points, of which the ij th entry is set as zero if the i th and j th data points are not among the k=10 nearest neighbours of each other. Ordering the data points based on their cohort memberships, we display the computed matrix W . W = AdjacencyMatrix(Data, neighbor=10, weight=0) def AdjacencyMatrix(Data, neighbor=10, weight=0): # Calculate the adjacency matrix of the input data points # # :param Data: input data point matrix, D times n with n: number of data samples # :param neighbor: the neighborhood parameter of the data sample, default is 10 # :param weight: whether the indices are real distance or indicators. 0: distance weight, 1: indicators # # :return: # W: the output Adjacency Matrix, n times n size Next, we look at the relations between data points and data cohorts. We compute a confidence matrix R . Its ij th element corresponds to a confidence value that the i th data point is from the j th data cohort. Ordering the data points based on their cohort membership, we display the computed matrix R . R = CohortConfidence(Data, DataCohort, lambda) def CohortConfidence(Data, DataCohort, lambda=0): # Calculate the cohort confidence matrix representing the confidence of each data sample belonging to a certain cohort # # :param Data: input data point matrix, D times n with n: number of data samples # :param DataCohort: input label column # :param lambda: the parameter that controls the confidence level of each data sample, default is 0 # # :return: # R: the output Confidence matrix, n times c size with c: number of data cohorts Finally, taking the cohort prototypes V , between-point adjacency matrix W and the cohort membership matrix R , as the input, COVA embeds the data points in the new space around the cohort prototypes, preserving information in W and R by solving a weighted sum objective function. The embedded data points are stored in the matrix Z . We display the embedded data points. result = COVA_embedding(Data, DataCohort, R, W, alpha=0.4, dim=2) def COVA_embedding(Data, DataCohort, R, W, alpha=0.4, dim=2): # The embedding process of ANGEL, finding the corresponding embedded data points in the low-dimensional space. # # :param Data: input data matrix, D times n with n: number of data samples # :param DataCohort: input label column # :param R: confidence matrix, default lambda = 0 # :param W: adjacency matrix, default with neighborhood = 10 # :param dim: the dimensionality of the embedded space, default dim = 2, the other choice is dim = 3 # :param alpha: 0: controls the balance between global and local visualizations, 0<alpha<1, where 0: the most local setting, 1: the most global setting # # :return # result: the embedded data points d times n with d: 2 or 3,, n: number of samples How to Run COVA Finally, the most important part. You don't need to break COVA into different components as above, but use it by a single command. Here is an example of how to run COVA to embed an example set of images in a 2-dimensional space. First we load the data. Data, DataCohort = loadData('OneFlower.mat') Then we run COVA, and display the embedded images. result = COVA(Data, DataCohort, lambda=0, neighbor=10, dim=2, alpha=0.4, typeCohort=0) def COVA(Data, DataCohort, lambda=0, neighbor=10, dim=2, alpha=0.4, typeCohort=0): # The embedding process of ANGEL, finding the corresponding embedded data points in the low-dimensional space. # # :param Data: input data matrix, D times n with D: feature dimension, n: number of data samples # :param DataCohort: input label column # :param lambda: the parameter that controls the confidence level of each data sample, default is 0 # :param neighbor: the neighborhood parameter of the data sample, default is 10 # :param dim: the dimensionality of the embedded space, default dim = 2, the other choice is dim = 3 # :param alpha: 0: controls the balance between global and local visualizations, 0<alpha<1, where 0: the most local setting, 1: the most global setting # :param typeCohort: whether to use the original data cohorts or generating new sub-cohorts. 0: original; 1: new sub-cohorts # # :return # result: the embedded data points d times n with d: 2 or 3,, n: number of samples","title":"COVA Tutorial"},{"location":"COVATutorial/#cova-tutorial","text":"We are going to show how COVA works by embedding an example set of 2-dimensional data points in a new 2D space. The goal is to preserve as well as possible the cohort structure of the data and the local neighbourhood relation between the individual data points.","title":"COVA Tutorial"},{"location":"COVATutorial/#how-cova-works","text":"First, we load the data. The example dataset contains n=1000 samples of d=2 dimensions belonging to c=7 cohorts. The coordinates of the data samples are stored in the matrix Data , while the cohort membership of each data sample is stored in DataCohort . DataInput, DataCohort = loadData('OneFlower.mat') def loadData(datapath): # The function will load the data from the local disk # :param datapath: could be direct path or indirect path, right now we only support '.mat' format. After loading the data, we preprocess the dataset by applying a certain scaler to the dataset. Here, we utilize the MinMax scaler to transform features by scaling each feature to a given range. Data = DataScaler(DataInput) def DataScaler(DataInput): # The function scales the dataset into a given range, here we fix the range to be [0,1] # :param DataInput: input data matrix in the high-dimensional space, D times n with D: feature dimension, n: number of data samples # :return: # Data: the scaled data matrix in the high-dimensional space, D times n with D: feature dimension, n: number of data samples Next, we estimate the Cohort Distance between any two data cohorts, and store the estimated distances in the matrix D_c . There are two different ways to calculate the cohort distance, which is selected by changing the hyper-parameter typeCohort . When typeCohort=0 , we keep the original data cohorts, and apply the cohort distance calculation. When typeCohort=1 , we divide each data cohorts into several sub-cohorts using K-means clustering algorithm , and calculate the cohort distance based on new generated cohorts. Dc, newlabel = ProtoGeneration(Data, DataCohort, typeCohort=0) def ProtoGeneration(Data, DataCohort, typeCohort=0): # Cohort distance calculation. # The default metric is based on the Euclidean distance. # # :param Data: input data matrix in the high-dimensional space, D times n with D: feature dimension, n: number of data samples # :param DataCohort: input label column # :param typeCohort: whether to use the original data cohorts or generating new sub-cohorts. 0: original; 1: new sub-cohorts # # :return: # Dc: the matrix with each indices being the distance between two cohorts # newlabel: keep the same to DataCohort when typeCohort=0, return the new generated data labels when typeCohort=1. Taking D_c as the input, we use an isometry embedding to generate cohort prototypes (one for each cohort), so that the Euclidean distances between two prototypes preserve the between-cohort distance values in D_c . Similarly, when typeCohort=0 , we will obtain c=7 prototypes which is identical to the number of original data cohort; when typeCohort=1 , the obtained prototypes are more than 7 , and here we fixing the c = \\lfloor0.1n\\rfloor . The computed prototype embeddings are stored in the matrix V . Here, the motivation of applying an isometric embedding algorithm for cohorts rather than individual points is that an algorithm of this type, e.g., multi-dimensional embedding and ordinal embedding, has better chance to succeed when handling a small number of instances. V0 = ProtoEmbedding(Dc, DataCohort) def ProtoEmbedding(Data, DataCohort, embedding = 'SOE'): # Prototypes embedding. # Finding the representative points of prototypes on 2d/3d spaces # # :param Dc: the matrix with each indices being the distance between two cohorts # :param DataCohort: input label column # # :return: # V: the embedded prototype points on the low-dimensional space We also scale our embedded prototypes to a certain range by using the DataScaler V= DataScaler(V0) Now, we look at the neighbouring relations between individual points. We compute an adjacency matrix W between the n=1000 points, of which the ij th entry is set as zero if the i th and j th data points are not among the k=10 nearest neighbours of each other. Ordering the data points based on their cohort memberships, we display the computed matrix W . W = AdjacencyMatrix(Data, neighbor=10, weight=0) def AdjacencyMatrix(Data, neighbor=10, weight=0): # Calculate the adjacency matrix of the input data points # # :param Data: input data point matrix, D times n with n: number of data samples # :param neighbor: the neighborhood parameter of the data sample, default is 10 # :param weight: whether the indices are real distance or indicators. 0: distance weight, 1: indicators # # :return: # W: the output Adjacency Matrix, n times n size Next, we look at the relations between data points and data cohorts. We compute a confidence matrix R . Its ij th element corresponds to a confidence value that the i th data point is from the j th data cohort. Ordering the data points based on their cohort membership, we display the computed matrix R . R = CohortConfidence(Data, DataCohort, lambda) def CohortConfidence(Data, DataCohort, lambda=0): # Calculate the cohort confidence matrix representing the confidence of each data sample belonging to a certain cohort # # :param Data: input data point matrix, D times n with n: number of data samples # :param DataCohort: input label column # :param lambda: the parameter that controls the confidence level of each data sample, default is 0 # # :return: # R: the output Confidence matrix, n times c size with c: number of data cohorts Finally, taking the cohort prototypes V , between-point adjacency matrix W and the cohort membership matrix R , as the input, COVA embeds the data points in the new space around the cohort prototypes, preserving information in W and R by solving a weighted sum objective function. The embedded data points are stored in the matrix Z . We display the embedded data points. result = COVA_embedding(Data, DataCohort, R, W, alpha=0.4, dim=2) def COVA_embedding(Data, DataCohort, R, W, alpha=0.4, dim=2): # The embedding process of ANGEL, finding the corresponding embedded data points in the low-dimensional space. # # :param Data: input data matrix, D times n with n: number of data samples # :param DataCohort: input label column # :param R: confidence matrix, default lambda = 0 # :param W: adjacency matrix, default with neighborhood = 10 # :param dim: the dimensionality of the embedded space, default dim = 2, the other choice is dim = 3 # :param alpha: 0: controls the balance between global and local visualizations, 0<alpha<1, where 0: the most local setting, 1: the most global setting # # :return # result: the embedded data points d times n with d: 2 or 3,, n: number of samples","title":"How COVA Works"},{"location":"COVATutorial/#how-to-run-cova","text":"Finally, the most important part. You don't need to break COVA into different components as above, but use it by a single command. Here is an example of how to run COVA to embed an example set of images in a 2-dimensional space. First we load the data. Data, DataCohort = loadData('OneFlower.mat') Then we run COVA, and display the embedded images. result = COVA(Data, DataCohort, lambda=0, neighbor=10, dim=2, alpha=0.4, typeCohort=0) def COVA(Data, DataCohort, lambda=0, neighbor=10, dim=2, alpha=0.4, typeCohort=0): # The embedding process of ANGEL, finding the corresponding embedded data points in the low-dimensional space. # # :param Data: input data matrix, D times n with D: feature dimension, n: number of data samples # :param DataCohort: input label column # :param lambda: the parameter that controls the confidence level of each data sample, default is 0 # :param neighbor: the neighborhood parameter of the data sample, default is 10 # :param dim: the dimensionality of the embedded space, default dim = 2, the other choice is dim = 3 # :param alpha: 0: controls the balance between global and local visualizations, 0<alpha<1, where 0: the most local setting, 1: the most global setting # :param typeCohort: whether to use the original data cohorts or generating new sub-cohorts. 0: original; 1: new sub-cohorts # # :return # result: the embedded data points d times n with d: 2 or 3,, n: number of samples","title":"How to Run COVA"},{"location":"Demo/","text":"Demo of the Algorithms Click here to jump to the DEMO website . The demo website illustrates examples embedded by these two algorithms under different hyper-parameter settings and supports users in visualizing their datasets. The fast-ANGEL version is applied to the users' custom datasets. The current restriction of input data format is '.mat'.","title":"Demo"},{"location":"Demo/#demo-of-the-algorithms","text":"Click here to jump to the DEMO website . The demo website illustrates examples embedded by these two algorithms under different hyper-parameter settings and supports users in visualizing their datasets. The fast-ANGEL version is applied to the users' custom datasets. The current restriction of input data format is '.mat'.","title":"Demo of the Algorithms"},{"location":"Examples/","text":"Examples Haven't been accomplished.","title":"Examples"},{"location":"Examples/#examples","text":"Haven't been accomplished.","title":"Examples"},{"location":"FunctionLists/","text":"API Reference ANGEL: AnchorPointGeneration AnchorEmbedding ANGEL_embedding COVA: PrototypeGeneration PrototypeEmbedding CohortConfidence COVAembedding Evaluation: Prev FunctionFile: loadData DataScaler CohortDistance AdjacencyMatrix localanchorembedding: AnchorGraph SOEembedding: SOE","title":"Lists"},{"location":"FunctionLists/#api-reference","text":"","title":"API Reference"},{"location":"FunctionLists/#angel","text":"AnchorPointGeneration AnchorEmbedding ANGEL_embedding","title":"ANGEL:"},{"location":"FunctionLists/#cova","text":"PrototypeGeneration PrototypeEmbedding CohortConfidence COVAembedding","title":"COVA:"},{"location":"FunctionLists/#evaluation","text":"Prev","title":"Evaluation:"},{"location":"FunctionLists/#functionfile","text":"loadData DataScaler CohortDistance AdjacencyMatrix","title":"FunctionFile:"},{"location":"FunctionLists/#localanchorembedding","text":"AnchorGraph","title":"localanchorembedding:"},{"location":"FunctionLists/#soeembedding","text":"SOE","title":"SOEembedding:"},{"location":"KeyKnowledges/","text":"Key Knowledge Lists adjacency matrix cohort distance cohort membership confidence matrix isometric embedding kmeans clustering Local Anchor Embedding Soft Ordinal Embedding","title":"Lists"},{"location":"KeyKnowledges/#key-knowledge-lists","text":"adjacency matrix cohort distance cohort membership confidence matrix isometric embedding kmeans clustering Local Anchor Embedding Soft Ordinal Embedding","title":"Key Knowledge Lists"},{"location":"PageUnderConstruction/","text":"This page will be published soon Click here to return to the Intro page.","title":"Examples"},{"location":"PageUnderConstruction/#this-page-will-be-published-soon","text":"Click here to return to the Intro page.","title":"This page will be published soon"},{"location":"FunctionList/AdjacencyMatrix/","text":"FunctionFile.AdjacencyMatrix AdjacencyMatrix(Data, neighbor=10, weight=1, metric='euclidean') Calculate the adjacency matrix of a given dataset. Parameters Format Description Data {matrix} of shape (n_samples, D_features) Note that we do not support the sparse matrix now. neighbor int, default = 10 The number of neighborhood points we would like to link within each data samples weight int, Whether to present the weighted matrix, when weight = 0; or a unweighted matrix, when weight = 1. metric str, default = 'euclidean' The metric to use when calculating distance between data samples. It must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter Returns Format Description W {matrix} of shape (n_samples, n_samples) A distance matrix D such that D_{ij} is the distance between the i -th and j -th vectors of the given matrix Data","title":"FunctionFile.AdjacencyMatrix"},{"location":"FunctionList/AdjacencyMatrix/#functionfileadjacencymatrix","text":"AdjacencyMatrix(Data, neighbor=10, weight=1, metric='euclidean') Calculate the adjacency matrix of a given dataset. Parameters Format Description Data {matrix} of shape (n_samples, D_features) Note that we do not support the sparse matrix now. neighbor int, default = 10 The number of neighborhood points we would like to link within each data samples weight int, Whether to present the weighted matrix, when weight = 0; or a unweighted matrix, when weight = 1. metric str, default = 'euclidean' The metric to use when calculating distance between data samples. It must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter Returns Format Description W {matrix} of shape (n_samples, n_samples) A distance matrix D such that D_{ij} is the distance between the i -th and j -th vectors of the given matrix Data","title":"FunctionFile.AdjacencyMatrix"},{"location":"FunctionList/AnchorGraph/","text":"localanchorembedding.AnchorGraph AnchorGraph(Data, Anchor, t=3, cn=10, metric='euclidean') Generates the graph matrix representing relationships between data samples and anchor points. Parameters Format Description Data {matrix} of shape (n_samples, D_features) Note that we do not support the sparse matrix now. Anchor {matrix} of shape (m_anchors, D_features) The input anchor points in the high-dimensional space. t int, default=3 The number of neighboring anchor points of each data sample. cn int, default=10 The number of iterations of the optimization process metric str, default = 'euclidean' The metric to use when calculating distance between data samples. It must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter Returns Format Description Z {matrix} of shape (n_samples, m_anchor) The AnchorGraph matrix storing relations between data samples and anchor points.","title":"localanchorembedding.AnchorGraph"},{"location":"FunctionList/AnchorGraph/#localanchorembeddinganchorgraph","text":"AnchorGraph(Data, Anchor, t=3, cn=10, metric='euclidean') Generates the graph matrix representing relationships between data samples and anchor points. Parameters Format Description Data {matrix} of shape (n_samples, D_features) Note that we do not support the sparse matrix now. Anchor {matrix} of shape (m_anchors, D_features) The input anchor points in the high-dimensional space. t int, default=3 The number of neighboring anchor points of each data sample. cn int, default=10 The number of iterations of the optimization process metric str, default = 'euclidean' The metric to use when calculating distance between data samples. It must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter Returns Format Description Z {matrix} of shape (n_samples, m_anchor) The AnchorGraph matrix storing relations between data samples and anchor points.","title":"localanchorembedding.AnchorGraph"},{"location":"FunctionList/COVAembedding/","text":"COVA.COVAembedding COVAembedding(Data, R, W, V, init=0, dim=2, alpha=0.4, T=50, COVAType='cova1', opttype='GD_Euclidean') Calculate embedded anchor points in the low-dimesional space. Parameters Format Description Data {matrix} of shape (n_samples, D_features) Note that we do not support the sparse matrix now. R {matrix} of shape (n_samples, c_cohorts) The ouput Confidence matrix W {matrix} of shape (n_samples, n_samples) A distance matrix D such that D_{ij} is the distance between the i -th and j -th vectors of the given matrix Data V {matrix} of shape (c_cohort, dim) The embedded prototype points in the low-dimensional space. init 0 or {matrix} of shape (n_samples, dim) Whether to fix the initialization manually. The default init=0 gives the random data points initialization of the optimization process. dim int, The dimensionality of the embedded space. alpha float, default=0.4 It controls the balance between global and local visualizations. 0 < alpha < 1, 0: the most local choice, 1: the most global choice. T int, default=50 The number of iterations of the optimization process COVAType str, default='cova1' Switch between different COVA types according to the paper. The choices are 'AnalyticalCOVA1', 'cova1', 'cova2', 'cova3', 'cova4'. optType str, default='GD_Euclidean' Switch between two different optimization process. The default 'constrained' refers to the optimization based on Euclidean space, the other choice 'GD_Riemannian' is based on the manifold optimization process. Note that 'GD_Riemannian' only works for 'cova1' currently. Returns Format Description result {matrix} of shape (n_sample, dim) The embedded data points in the low-dimensional space.","title":"COVA.COVAembedding"},{"location":"FunctionList/COVAembedding/#covacovaembedding","text":"COVAembedding(Data, R, W, V, init=0, dim=2, alpha=0.4, T=50, COVAType='cova1', opttype='GD_Euclidean') Calculate embedded anchor points in the low-dimesional space. Parameters Format Description Data {matrix} of shape (n_samples, D_features) Note that we do not support the sparse matrix now. R {matrix} of shape (n_samples, c_cohorts) The ouput Confidence matrix W {matrix} of shape (n_samples, n_samples) A distance matrix D such that D_{ij} is the distance between the i -th and j -th vectors of the given matrix Data V {matrix} of shape (c_cohort, dim) The embedded prototype points in the low-dimensional space. init 0 or {matrix} of shape (n_samples, dim) Whether to fix the initialization manually. The default init=0 gives the random data points initialization of the optimization process. dim int, The dimensionality of the embedded space. alpha float, default=0.4 It controls the balance between global and local visualizations. 0 < alpha < 1, 0: the most local choice, 1: the most global choice. T int, default=50 The number of iterations of the optimization process COVAType str, default='cova1' Switch between different COVA types according to the paper. The choices are 'AnalyticalCOVA1', 'cova1', 'cova2', 'cova3', 'cova4'. optType str, default='GD_Euclidean' Switch between two different optimization process. The default 'constrained' refers to the optimization based on Euclidean space, the other choice 'GD_Riemannian' is based on the manifold optimization process. Note that 'GD_Riemannian' only works for 'cova1' currently. Returns Format Description result {matrix} of shape (n_sample, dim) The embedded data points in the low-dimensional space.","title":"COVA.COVAembedding"},{"location":"FunctionList/CohortConfidence/","text":"COVA.CohortConfidence CohortConfidence(Data, DataCohort, lamb=0) Calculate the cohort confidence matrix representing the confidence of each data sample belonging to a certain cohort. Parameters Format Description Data {matrix} of shape (n_samples, D_features) Note that we do not support the sparse matrix now. DataCohort {column} of shape (n_samples, 1) The corresponding labels of each data sample lamb float, default=0 The parameter that controls the confidence level of each data sample, default is 0 Returns Format Description R {matrix} of shape (n_samples, c_cohorts) The ouput Confidence matrix","title":"COVA.CohortConfidence"},{"location":"FunctionList/CohortConfidence/#covacohortconfidence","text":"CohortConfidence(Data, DataCohort, lamb=0) Calculate the cohort confidence matrix representing the confidence of each data sample belonging to a certain cohort. Parameters Format Description Data {matrix} of shape (n_samples, D_features) Note that we do not support the sparse matrix now. DataCohort {column} of shape (n_samples, 1) The corresponding labels of each data sample lamb float, default=0 The parameter that controls the confidence level of each data sample, default is 0 Returns Format Description R {matrix} of shape (n_samples, c_cohorts) The ouput Confidence matrix","title":"COVA.CohortConfidence"},{"location":"FunctionList/CohortDistance/","text":"FunctionFile.CohortDistance CohortDistance(Data, DataLabel, CohortMetric='average', metricC='euclidean') Calculate the adjacency matrix of a given dataset. Parameters Format Description Data {matrix} of shape (n_samples, D_features) Note that we do not support the sparse matrix now. DataLabel {column} of shape (n_samples, 1) The corresponding labels of each data sample CohortMetric str, default = 'average' Different approaches for calculating the cohort distance, the options are: 'single', 'complete', 'centroid', 'Hausdoff'. metric str, default = 'euclidean' The metric to use when calculating distance between data samples. It must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter Returns Format Description dCluster {matrix} of shape (c_cohort, c_cohort) A cohort distance matrix dCluster such that each indice is the distance between two data cohorts","title":"FunctionFile.CohortDistance"},{"location":"FunctionList/CohortDistance/#functionfilecohortdistance","text":"CohortDistance(Data, DataLabel, CohortMetric='average', metricC='euclidean') Calculate the adjacency matrix of a given dataset. Parameters Format Description Data {matrix} of shape (n_samples, D_features) Note that we do not support the sparse matrix now. DataLabel {column} of shape (n_samples, 1) The corresponding labels of each data sample CohortMetric str, default = 'average' Different approaches for calculating the cohort distance, the options are: 'single', 'complete', 'centroid', 'Hausdoff'. metric str, default = 'euclidean' The metric to use when calculating distance between data samples. It must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter Returns Format Description dCluster {matrix} of shape (c_cohort, c_cohort) A cohort distance matrix dCluster such that each indice is the distance between two data cohorts","title":"FunctionFile.CohortDistance"},{"location":"FunctionList/DataScaler/","text":"FunctionFile.DataScaler Haven't been accomplished","title":"FunctionFile.DataScaler"},{"location":"FunctionList/DataScaler/#functionfiledatascaler","text":"Haven't been accomplished","title":"FunctionFile.DataScaler"},{"location":"FunctionList/PrototypeEmbedding/","text":"COVA.PrototypeEmbedding PrototypeEmbedding(Dc, DataLabel, dim=2, Embedding='SOE', init=0) Calculate embedded prototypes in the low-dimesional space. Parameters Format Description Dc {matrix} of shape (c_cohort, c_cohort) A cohort distance matrix dCluster such that each indice is the distance between two data cohorts DataLabel {column} of shape (n_samples, 1) The corresponding labels of each data sample dim int, The dimensionality of the embedded space. Embedding str, default='SOE' Switch between different isometric embedding approaches. The options are 'SOE', 'MDS', and 'Isomap'. init 0 or {matrix} of shape (n_samples, dim) Whether to fix the initialization manually. The default init=0 gives the random prototypes initialization of the optimization process. Returns Format Description V {matrix} of shape (c_cohort, dim) The embedded prototypes in the low-dimensional space.","title":"COVA.PrototypeEmbedding"},{"location":"FunctionList/PrototypeEmbedding/#covaprototypeembedding","text":"PrototypeEmbedding(Dc, DataLabel, dim=2, Embedding='SOE', init=0) Calculate embedded prototypes in the low-dimesional space. Parameters Format Description Dc {matrix} of shape (c_cohort, c_cohort) A cohort distance matrix dCluster such that each indice is the distance between two data cohorts DataLabel {column} of shape (n_samples, 1) The corresponding labels of each data sample dim int, The dimensionality of the embedded space. Embedding str, default='SOE' Switch between different isometric embedding approaches. The options are 'SOE', 'MDS', and 'Isomap'. init 0 or {matrix} of shape (n_samples, dim) Whether to fix the initialization manually. The default init=0 gives the random prototypes initialization of the optimization process. Returns Format Description V {matrix} of shape (c_cohort, dim) The embedded prototypes in the low-dimensional space.","title":"COVA.PrototypeEmbedding"},{"location":"FunctionList/PrototypeGeneration/","text":"COVA.PrototypeGeneration ProtoGeneration(Data, DataLabel, C=1, metric='euclidean') Generate relations between prototypes in the high-dimensional space. Parameters Format Description Data {matrix} of shape (n_samples, D_features) Note that we do not support the sparse matrix now. DataLabel {column} of shape (n_samples, 1) The corresponding labels of each data sample C int, Whether to use the original data cohort to generate prototypes. When default C=1, we further clustering the dataset into c_cohort = \\lfloor0.1n\\rfloor cohorts. When C=0, we keep the original data cohorts. metric str, default = 'euclidean' The metric to use when calculating distance between data samples. It must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter Returns Format Description Dc {matrix} of shape (c_cohort, c_cohort) A cohort distance matrix dCluster such that each indice is the distance between two data cohorts","title":"COVA.PrototypeGeneration"},{"location":"FunctionList/PrototypeGeneration/#covaprototypegeneration","text":"ProtoGeneration(Data, DataLabel, C=1, metric='euclidean') Generate relations between prototypes in the high-dimensional space. Parameters Format Description Data {matrix} of shape (n_samples, D_features) Note that we do not support the sparse matrix now. DataLabel {column} of shape (n_samples, 1) The corresponding labels of each data sample C int, Whether to use the original data cohort to generate prototypes. When default C=1, we further clustering the dataset into c_cohort = \\lfloor0.1n\\rfloor cohorts. When C=0, we keep the original data cohorts. metric str, default = 'euclidean' The metric to use when calculating distance between data samples. It must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter Returns Format Description Dc {matrix} of shape (c_cohort, c_cohort) A cohort distance matrix dCluster such that each indice is the distance between two data cohorts","title":"COVA.PrototypeGeneration"},{"location":"FunctionList/SOE/","text":"ANGEL.ANGEL_embedding SOE(Matrix, DataLabel, C=0, anchor=0, dim=2, init=0, metric='euclidean', flagMove=0, T=50, scale=1) Calculate embedded anchor points in the low-dimesional space. Parameters Format Description Matrix {matrix} of shape (n_samples, n_samples) A distance matrix D such that D_{ij} is the indicator whether j -th sample is among the neighborhood of the i -th sample of the given matrix Data DataLabel {column} of shape (n_samples, 1) The corresponding labels of each data sample cinit {matrix} of shape (c_cohorts, dim) Note that it only has the effect when flagMove=1. The embedded cohort positions in the low-dimensional space. anchor {matrix} of shape (m_anchors, dim) Note that it only has the effect when flagMove=1. The oringal anchor points before applying the position refinement process. dim int, The dimensionality of the embedded space. init 0 or {matrix} of shape (n_samples, dim) Whether to fix the initialization manually. The default init=0 gives the random data points initialization of the optimization process. metric str, default = 'euclidean' The metric to use when calculating distance between data samples. It must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter flagMove int, Whether to refine the position of the embedded anchor points. The default flagMove=0 keeps the original positions of embedded anchor points T int, default=50 The number of iterations of the optimization process scale float, default = 1.0 Note that it only has the effect when flagMove=1. In that case, the scale parameter controls the rescaling of each data cohort. Returns Format Description result {matrix} of shape (n_sample, dim) The embedded data points in the low-dimensional space.","title":"SOEembedding.SOE"},{"location":"FunctionList/SOE/#angelangel_embedding","text":"SOE(Matrix, DataLabel, C=0, anchor=0, dim=2, init=0, metric='euclidean', flagMove=0, T=50, scale=1) Calculate embedded anchor points in the low-dimesional space. Parameters Format Description Matrix {matrix} of shape (n_samples, n_samples) A distance matrix D such that D_{ij} is the indicator whether j -th sample is among the neighborhood of the i -th sample of the given matrix Data DataLabel {column} of shape (n_samples, 1) The corresponding labels of each data sample cinit {matrix} of shape (c_cohorts, dim) Note that it only has the effect when flagMove=1. The embedded cohort positions in the low-dimensional space. anchor {matrix} of shape (m_anchors, dim) Note that it only has the effect when flagMove=1. The oringal anchor points before applying the position refinement process. dim int, The dimensionality of the embedded space. init 0 or {matrix} of shape (n_samples, dim) Whether to fix the initialization manually. The default init=0 gives the random data points initialization of the optimization process. metric str, default = 'euclidean' The metric to use when calculating distance between data samples. It must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter flagMove int, Whether to refine the position of the embedded anchor points. The default flagMove=0 keeps the original positions of embedded anchor points T int, default=50 The number of iterations of the optimization process scale float, default = 1.0 Note that it only has the effect when flagMove=1. In that case, the scale parameter controls the rescaling of each data cohort. Returns Format Description result {matrix} of shape (n_sample, dim) The embedded data points in the low-dimensional space.","title":"ANGEL.ANGEL_embedding"},{"location":"FunctionList/loadData/","text":"FunctionFile.loadData Haven't been accomplished","title":"FunctionFile.loadData"},{"location":"FunctionList/loadData/#functionfileloaddata","text":"Haven't been accomplished","title":"FunctionFile.loadData"},{"location":"KeyKnowledge/adjacencymatrix/","text":"Adjacency Matrix Given a set of data sample \\mathbf{X}=\\{\\mathbf{x}_1, \\mathbf{x}_2,\\dots, \\mathbf{x}_n\\} , the adjacency matrix of this dataset is a square n\\times n matrix \\mathbf{W} where each element w_{ij} representing the connection information between the data sample \\mathbf{x}_i with the data sample \\mathbf{x}_j . The w_{ij} is bigger than 0 when there is an edge between these two data samples while w_{ij} is 0 when there is no edge. The diagonal elements w_{ii} are set to be zeros since there is no edge from the data sample to itself. Our adjacency matrix is built on the graph connecting the k -nearest neighborhood samples with each data sample. That means, an edge w_{ij} exists if the data sample \\mathbf{x}_j is among the k -nearest neighborhood points of the data sample \\mathbf{x}_i , otherwise there is no edge. If the graph is directed, the fact that the data sample \\mathbf{x}_j is among the k -nearest neighborhood points of the data sample \\mathbf{x}_i will not guarantee that data sample \\mathbf{x}_i is also among the k -nearest neighborhood points of the data sample \\mathbf{x}_j , which will lead to w_{ij}\\neq w_{ji} . If the graph is undirected, then w_{ij} = w_{ji} as we will make sure that both samples are in the neighborhood of the other one. If the graph is weighted, the indice w_{ij} can take the real distance value as the weight of the edge. The actual weight w_{ij} can be computed using any predefined similarity measure, such as the Gaussian kernel, cosine similarity, etc. The unweighted graph only has w_{ij} = 1 as the indicator of all edges. An example of the undirected weighted adjacency matrix based on 3-nearest neighborhood graph is as the following: \\mathbf{W} = \\begin{bmatrix}0& 1.2& 0& 1.4& 0.8& 0 \\\\1.2& 0& 0.6& 0.7& 0& 0 \\\\ 0& 0.6& 0& 1.0& 0& 1.5\\\\ 0& 0.7& 1.0& 0& 1.2& 0& \\\\ 0.8& 0& 0& 1.2& 0& 1.0\\\\ 1.6& 0& 1.5& 0& 1.0& 0 \\end{bmatrix} As for a real-world dataset, the adjacency matrix of the daset can be viewed as a heatmap where the temperature denotes the numerical value of weights. Here are two heatmap examples of weighted adjacency matrix and the unweighted adjacency matrix.","title":"adjancency matrix"},{"location":"KeyKnowledge/adjacencymatrix/#adjacency-matrix","text":"Given a set of data sample \\mathbf{X}=\\{\\mathbf{x}_1, \\mathbf{x}_2,\\dots, \\mathbf{x}_n\\} , the adjacency matrix of this dataset is a square n\\times n matrix \\mathbf{W} where each element w_{ij} representing the connection information between the data sample \\mathbf{x}_i with the data sample \\mathbf{x}_j . The w_{ij} is bigger than 0 when there is an edge between these two data samples while w_{ij} is 0 when there is no edge. The diagonal elements w_{ii} are set to be zeros since there is no edge from the data sample to itself. Our adjacency matrix is built on the graph connecting the k -nearest neighborhood samples with each data sample. That means, an edge w_{ij} exists if the data sample \\mathbf{x}_j is among the k -nearest neighborhood points of the data sample \\mathbf{x}_i , otherwise there is no edge. If the graph is directed, the fact that the data sample \\mathbf{x}_j is among the k -nearest neighborhood points of the data sample \\mathbf{x}_i will not guarantee that data sample \\mathbf{x}_i is also among the k -nearest neighborhood points of the data sample \\mathbf{x}_j , which will lead to w_{ij}\\neq w_{ji} . If the graph is undirected, then w_{ij} = w_{ji} as we will make sure that both samples are in the neighborhood of the other one. If the graph is weighted, the indice w_{ij} can take the real distance value as the weight of the edge. The actual weight w_{ij} can be computed using any predefined similarity measure, such as the Gaussian kernel, cosine similarity, etc. The unweighted graph only has w_{ij} = 1 as the indicator of all edges. An example of the undirected weighted adjacency matrix based on 3-nearest neighborhood graph is as the following: \\mathbf{W} = \\begin{bmatrix}0& 1.2& 0& 1.4& 0.8& 0 \\\\1.2& 0& 0.6& 0.7& 0& 0 \\\\ 0& 0.6& 0& 1.0& 0& 1.5\\\\ 0& 0.7& 1.0& 0& 1.2& 0& \\\\ 0.8& 0& 0& 1.2& 0& 1.0\\\\ 1.6& 0& 1.5& 0& 1.0& 0 \\end{bmatrix} As for a real-world dataset, the adjacency matrix of the daset can be viewed as a heatmap where the temperature denotes the numerical value of weights. Here are two heatmap examples of weighted adjacency matrix and the unweighted adjacency matrix.","title":"Adjacency Matrix"},{"location":"KeyKnowledge/cohortdistance/","text":"Cohort Distance The cohort distance measures the proximity information betwee cohorts of a given dataset \\mathbf{X}=\\{\\mathbf{x}_1, \\mathbf{x}_2,\\dots, \\mathbf{x}_n\\} . Assume there are c different cohorts in the dataset \\mathbf{X} , we construct a c\\times c cohort distance matrix \\mathbf{D}_c where each indice d_{ij} represent the proximity information between cohort c_i and cohort c_j . The most straightforward way to compute d_{ij} is to calculate the distance between cohort centers where d_{ij} = d\\left(\\frac{1}{n_{i}}\\sum_{y_p = i}\\mathbf{x}_p, \\frac{1}{n_{j}}\\sum_{y_q = j}\\mathbf{x}_q\\right). Here, d(\\cdot,\\cdot) denotes any distance measures between two data samples, which can be the Euclidean distance(which we applied to our algorithms), a Gaussian kernel, etc. Another popular way of generating cohort distance is to find the average-linkage distance which is based on calculating the average sample pair relations, \\mathbf{d}_{ij} = \\frac{1}{n_i n_j} \\sum_{{y}_{p}=i}\\sum_{{y}_{q}=j} d(\\mathbf{x}_{p},\\mathbf{x}_{q}). Moreover, the adjacency matrix \\mathbf{W} generated for the local neighbor preservation can be used to adjust the average distance between cohorts as d_{i j} \\leftarrow \\frac{1}{n_{i} n_{j}} \\sum_{y_{p}=i, y_{q}=j} w_{p q}, which reflects density of neighbor link between samples from two cohorts. More generally, the users can define their own cohort similarity measures according to the applications of different datasets. One example is that, when visualizing images based on their pixel content (e.g., using a convolutional neural network (CNN) to construct \\mathbf{X} ), proximity between image cohorts can be computed externally by examining the image captions. For instance, by treating the collection of image captions from each cohort as a document and modelling the documents using a bag-of-word model, proximity between two cohorts can be computed as the cosine similarity between their two document vectors.","title":"cohort distance"},{"location":"KeyKnowledge/cohortdistance/#cohort-distance","text":"The cohort distance measures the proximity information betwee cohorts of a given dataset \\mathbf{X}=\\{\\mathbf{x}_1, \\mathbf{x}_2,\\dots, \\mathbf{x}_n\\} . Assume there are c different cohorts in the dataset \\mathbf{X} , we construct a c\\times c cohort distance matrix \\mathbf{D}_c where each indice d_{ij} represent the proximity information between cohort c_i and cohort c_j . The most straightforward way to compute d_{ij} is to calculate the distance between cohort centers where d_{ij} = d\\left(\\frac{1}{n_{i}}\\sum_{y_p = i}\\mathbf{x}_p, \\frac{1}{n_{j}}\\sum_{y_q = j}\\mathbf{x}_q\\right). Here, d(\\cdot,\\cdot) denotes any distance measures between two data samples, which can be the Euclidean distance(which we applied to our algorithms), a Gaussian kernel, etc. Another popular way of generating cohort distance is to find the average-linkage distance which is based on calculating the average sample pair relations, \\mathbf{d}_{ij} = \\frac{1}{n_i n_j} \\sum_{{y}_{p}=i}\\sum_{{y}_{q}=j} d(\\mathbf{x}_{p},\\mathbf{x}_{q}). Moreover, the adjacency matrix \\mathbf{W} generated for the local neighbor preservation can be used to adjust the average distance between cohorts as d_{i j} \\leftarrow \\frac{1}{n_{i} n_{j}} \\sum_{y_{p}=i, y_{q}=j} w_{p q}, which reflects density of neighbor link between samples from two cohorts. More generally, the users can define their own cohort similarity measures according to the applications of different datasets. One example is that, when visualizing images based on their pixel content (e.g., using a convolutional neural network (CNN) to construct \\mathbf{X} ), proximity between image cohorts can be computed externally by examining the image captions. For instance, by treating the collection of image captions from each cohort as a document and modelling the documents using a bag-of-word model, proximity between two cohorts can be computed as the cosine similarity between their two document vectors.","title":"Cohort Distance"},{"location":"KeyKnowledge/cohortmembership/","text":"Cohort Membership The cohort membership refers to a column storing information about which cohort should each data sample belong to. The vector \\mathbf{y}=[y_1, y_2, \\dots, y_n]^T with y_i\\in\\{1,2,\\dots,c\\} indicating the cohort index of i -th sample, and c is the total number of the existing cohort. The most straightforward way is to retrieve the label information from the given dataset or obtained by a classification problem. However, if labels are not given, the cohort membership should be generated before applying the embedding algorithms. Regarding data cohorts, one way of generating cohort membership is to apply clustering algorithms which should assign each data sample to a certain cluster. The kmeans clustering is commonly used. Other clustering methods like spectral clustering, agglomerative hierarchical clustering, can also be used.","title":"cohort membership"},{"location":"KeyKnowledge/cohortmembership/#cohort-membership","text":"The cohort membership refers to a column storing information about which cohort should each data sample belong to. The vector \\mathbf{y}=[y_1, y_2, \\dots, y_n]^T with y_i\\in\\{1,2,\\dots,c\\} indicating the cohort index of i -th sample, and c is the total number of the existing cohort. The most straightforward way is to retrieve the label information from the given dataset or obtained by a classification problem. However, if labels are not given, the cohort membership should be generated before applying the embedding algorithms. Regarding data cohorts, one way of generating cohort membership is to apply clustering algorithms which should assign each data sample to a certain cluster. The kmeans clustering is commonly used. Other clustering methods like spectral clustering, agglomerative hierarchical clustering, can also be used.","title":"Cohort Membership"},{"location":"KeyKnowledge/confidencematrix/","text":"Confidence Matrix The confidence matrix \\mathbf{R}=\\{r_{ij}\\} is generated with each element quantifies the degree of confidence that the i -th sample belongs to the j -th cohort. The template confidence r_{ij} to be matched in the target space can be computed by examining the proximity between the i -th sample and the centroid of the j -th cohort in the original space, such that r_{i j}=r\\left(\\boldsymbol{x}_{i}, \\frac{1}{n_{j}} \\sum_{y_{s}=j} \\boldsymbol{x}_{s}\\right), where r(\\cdot,\\cdot) is set as a similarity measure. It can also be computed as the averaged adjacency weights in \\mathbf{W} between the i -th sample and samples from the j th cohort, given as r_{i j}=\\left\\{\\begin{array}{ll} \\frac{1}{n_{j}-1} \\sum_{s \\neq i, y_{s}=j} w_{i s}, & \\text { if } y_{i}=j \\\\ \\frac{1}{n_{j}} \\sum_{y_{s}=j} w_{i s}, & \\text { otherwise } \\end{array}\\right. The above formulations of r_{ij} push data samples that are closer to a cohort center in the original space to stay closer to its corresponding cohort prototype in the target space. To enhance cohort separation, the links between a data sample and prototypes can be adjusted based on the following rescaling process \\mathbf{R} \\leftarrow\\left\\{\\begin{array}{ll} \\mathfrak{L}\\left(\\mathbf{R} \\circ \\mathbf{Y},\\left[1-\\lambda_{p}, 1\\right]\\right)+\\mathfrak{L}\\left(\\mathbf{R} \\circ(1-\\mathbf{Y}),\\left[0, \\lambda_{p}\\right]\\right), & \\text { if } \\lambda_{p} \\neq 0 \\\\ \\mathbf{R} \\circ \\mathbf{Y}, & \\text { if } \\lambda_{p}=0 \\end{array}\\right. where 0 \\leq \\lambda_p \\ll 1 and \\mathcal{L}(\\cdot,[a,b]) is a mapping function that linearly rescales elements in the input matrix to a given interval [a, b] . When \\lambda_p = 0, cohort separation is maximally enforced by simply cutting the links between a data sample and its irrelevant prototypes by fixing r_{ij} to zero when y_i \\neq j . An alternative for computing the confidence degree is to formulate r_{ij} as a probability, according to r_{i j}=\\left\\{\\begin{array}{ll} p\\left(s_{i j}\\right), & \\text { if } y_{i}=j \\\\ \\lambda_{p}, & \\text { otherwise } \\end{array}\\right. Between samples and their irrelevant cohorts, r_{ij} is set as a small value 0 \\leq \\lambda_p \\ll 1 to equally push the samples to stay away. Between a sample and its own cohort, r_{ij} is obtained using a probability function p(\\cdot) estimated from the similarity s_{i j} which is the original confidence value calculated from the first two equations. The normal distribution, Student\u2019s t-distribution, and other complex alternatives can be applied.","title":"confidence matrix"},{"location":"KeyKnowledge/confidencematrix/#confidence-matrix","text":"The confidence matrix \\mathbf{R}=\\{r_{ij}\\} is generated with each element quantifies the degree of confidence that the i -th sample belongs to the j -th cohort. The template confidence r_{ij} to be matched in the target space can be computed by examining the proximity between the i -th sample and the centroid of the j -th cohort in the original space, such that r_{i j}=r\\left(\\boldsymbol{x}_{i}, \\frac{1}{n_{j}} \\sum_{y_{s}=j} \\boldsymbol{x}_{s}\\right), where r(\\cdot,\\cdot) is set as a similarity measure. It can also be computed as the averaged adjacency weights in \\mathbf{W} between the i -th sample and samples from the j th cohort, given as r_{i j}=\\left\\{\\begin{array}{ll} \\frac{1}{n_{j}-1} \\sum_{s \\neq i, y_{s}=j} w_{i s}, & \\text { if } y_{i}=j \\\\ \\frac{1}{n_{j}} \\sum_{y_{s}=j} w_{i s}, & \\text { otherwise } \\end{array}\\right. The above formulations of r_{ij} push data samples that are closer to a cohort center in the original space to stay closer to its corresponding cohort prototype in the target space. To enhance cohort separation, the links between a data sample and prototypes can be adjusted based on the following rescaling process \\mathbf{R} \\leftarrow\\left\\{\\begin{array}{ll} \\mathfrak{L}\\left(\\mathbf{R} \\circ \\mathbf{Y},\\left[1-\\lambda_{p}, 1\\right]\\right)+\\mathfrak{L}\\left(\\mathbf{R} \\circ(1-\\mathbf{Y}),\\left[0, \\lambda_{p}\\right]\\right), & \\text { if } \\lambda_{p} \\neq 0 \\\\ \\mathbf{R} \\circ \\mathbf{Y}, & \\text { if } \\lambda_{p}=0 \\end{array}\\right. where 0 \\leq \\lambda_p \\ll 1 and \\mathcal{L}(\\cdot,[a,b]) is a mapping function that linearly rescales elements in the input matrix to a given interval [a, b] . When \\lambda_p = 0, cohort separation is maximally enforced by simply cutting the links between a data sample and its irrelevant prototypes by fixing r_{ij} to zero when y_i \\neq j . An alternative for computing the confidence degree is to formulate r_{ij} as a probability, according to r_{i j}=\\left\\{\\begin{array}{ll} p\\left(s_{i j}\\right), & \\text { if } y_{i}=j \\\\ \\lambda_{p}, & \\text { otherwise } \\end{array}\\right. Between samples and their irrelevant cohorts, r_{ij} is set as a small value 0 \\leq \\lambda_p \\ll 1 to equally push the samples to stay away. Between a sample and its own cohort, r_{ij} is obtained using a probability function p(\\cdot) estimated from the similarity s_{i j} which is the original confidence value calculated from the first two equations. The normal distribution, Student\u2019s t-distribution, and other complex alternatives can be applied.","title":"Confidence Matrix"},{"location":"KeyKnowledge/isometricembedding/","text":"Isometirc embedding In Riemannian geometry, let (M, g) and (N, h) be Riemannian manifolds, an isometric embedding is a smooth embedding f : M \\rightarrow N which preserves the metric in the sense that g is equal to the pullback of h by f , i.e. g = f\\times h . In our visualization situation, the pullback can be treated as a map from the high-dimensional manifold to a low-dimensional manifold. To make it simple, the isometric embedding preserves all the distance/ordinal information obtained from the dataset in the high-dimensional space while doing the embedding process. In reality, it is not possible to isometrically embed any data with an intrinsic dimensionality greater than 3 to a 2,3-D spaces. However, approaches like multi-dimensional scaling (MDS), soft ordinal embedding(SOE) are typical algorithms to preserve the approximate isometry of the dataset.","title":"isometric embedding"},{"location":"KeyKnowledge/isometricembedding/#isometirc-embedding","text":"In Riemannian geometry, let (M, g) and (N, h) be Riemannian manifolds, an isometric embedding is a smooth embedding f : M \\rightarrow N which preserves the metric in the sense that g is equal to the pullback of h by f , i.e. g = f\\times h . In our visualization situation, the pullback can be treated as a map from the high-dimensional manifold to a low-dimensional manifold. To make it simple, the isometric embedding preserves all the distance/ordinal information obtained from the dataset in the high-dimensional space while doing the embedding process. In reality, it is not possible to isometrically embed any data with an intrinsic dimensionality greater than 3 to a 2,3-D spaces. However, approaches like multi-dimensional scaling (MDS), soft ordinal embedding(SOE) are typical algorithms to preserve the approximate isometry of the dataset.","title":"Isometirc embedding"},{"location":"KeyKnowledge/kmeansclustering/","text":"K-means Clustering The objective of the K-means Clustering algorithm is to group similar data samples together and observing underlying data patterns. To achieve this goal, the algorithm tries to find a fixed number of k data clusters, where each cluster is a group of data samples share certain similarities. To process the data, the K-means clustering algorithm starts with k randomly selected centroids, which are used as the beginning points for every cluster, and then performs iterative calculations to optimize the positions of the centroids. One can refer to the sklearn.cluster.KMeans to learn how to use the algorithm in python.","title":"kmeans clustering"},{"location":"KeyKnowledge/kmeansclustering/#k-means-clustering","text":"The objective of the K-means Clustering algorithm is to group similar data samples together and observing underlying data patterns. To achieve this goal, the algorithm tries to find a fixed number of k data clusters, where each cluster is a group of data samples share certain similarities. To process the data, the K-means clustering algorithm starts with k randomly selected centroids, which are used as the beginning points for every cluster, and then performs iterative calculations to optimize the positions of the centroids. One can refer to the sklearn.cluster.KMeans to learn how to use the algorithm in python.","title":"K-means Clustering"},{"location":"KeyKnowledge/localAnchorEmbedding/","text":"Local Anchor Embedding Anchor points are a small set of data points to roughly represent a data distribution or a graph structure, e.g., by using a clustering algorithm and computing the cluster centers. Each anchor point is denoted by \\mathbf{u}_i \\in \\mathbb{R}^D , and stored in anchor point matrix \\mathbf{U} = \\{\\mathbf{u}_1,\\dots,\\mathbf{u}_m\\}\\subset \\mathbb{R}^{m\\times D} . Local Anchor Embedding attempts to approximate each observed data point by a convex combination of its t -nearest anchor points ( t -NAP). Let \\mathbf{w}_i=[w_{i1}, w_{i2}, \\ldots, w_{im}]^T denote the combination weights for reconstructing the i -th data point, and \\mathbf{W} = \\{\\mathbf{w}_i\\} is the reconstruction matrix. It is optimized by solving \\mathbf{w}_{i}^{*}=\\arg \\min _{\\mathbf{w}_{i} \\in \\mathbb{R}^{m}} \\frac{1}{2}\\left\\|\\mathbf{x}_{i}-\\mathbf{U}^{T} \\mathbf{w}_{i}\\right\\|_{2}^{2} subject to \\mathbf{1}^{T} \\mathbf{w}_{i}=1, w_{i j} \\geq 0, w_{i j}=0 if \\mathbf{u}_{j} \\notin t-\\operatorname{NAP}\\left(\\mathbf{x}_{i}\\right) where \\|\\cdot\\|_2 denotes the l_2 norm. The Nesterov\u2019s method is used for optimization. It alternates between gradient update and proper extrapolation for acceleration purpose.","title":"Local Anchor Embedding"},{"location":"KeyKnowledge/localAnchorEmbedding/#local-anchor-embedding","text":"Anchor points are a small set of data points to roughly represent a data distribution or a graph structure, e.g., by using a clustering algorithm and computing the cluster centers. Each anchor point is denoted by \\mathbf{u}_i \\in \\mathbb{R}^D , and stored in anchor point matrix \\mathbf{U} = \\{\\mathbf{u}_1,\\dots,\\mathbf{u}_m\\}\\subset \\mathbb{R}^{m\\times D} . Local Anchor Embedding attempts to approximate each observed data point by a convex combination of its t -nearest anchor points ( t -NAP). Let \\mathbf{w}_i=[w_{i1}, w_{i2}, \\ldots, w_{im}]^T denote the combination weights for reconstructing the i -th data point, and \\mathbf{W} = \\{\\mathbf{w}_i\\} is the reconstruction matrix. It is optimized by solving \\mathbf{w}_{i}^{*}=\\arg \\min _{\\mathbf{w}_{i} \\in \\mathbb{R}^{m}} \\frac{1}{2}\\left\\|\\mathbf{x}_{i}-\\mathbf{U}^{T} \\mathbf{w}_{i}\\right\\|_{2}^{2} subject to \\mathbf{1}^{T} \\mathbf{w}_{i}=1, w_{i j} \\geq 0, w_{i j}=0 if \\mathbf{u}_{j} \\notin t-\\operatorname{NAP}\\left(\\mathbf{x}_{i}\\right) where \\|\\cdot\\|_2 denotes the l_2 norm. The Nesterov\u2019s method is used for optimization. It alternates between gradient update and proper extrapolation for acceleration purpose.","title":"Local Anchor Embedding"},{"location":"KeyKnowledge/softOrdinalEmbedding/","text":"Soft Ordinal Embedding Soft Ordianl Embedding is a typical ordinal embedding method. It constructs the embedded data by enforcing ordinal constraints derived from the original data. Given a triplet of data points (\\mathbf{x}_i, \\mathbf{x}_j, \\mathbf{x}_l) , the ordinal constraint of the triplet is settled as \\text{If } D(\\mathbf{x}_i,\\mathbf{x}_j) < D(\\mathbf{x}_i,\\mathbf{x}_l), \\text{ then } \\hat{D} (\\hat{\\mathbf{x}}_i,\\hat{\\mathbf{x}}_j) < \\hat{D} (\\hat{\\mathbf{x}}_i,\\hat{\\mathbf{x}}_l). Here D(\\cdot,\\cdot) denotes the distance between two data points in the original space, while \\hat{D}(\\cdot,\\cdot) denotes the distance in the embedded space. In the non-metric embedding setting, the value of D(\\mathbf{x}_i,\\mathbf{x}_j) is unknown but the distance order reflected by the left inequality of the above equation is known. The triplet set, as \\Gamma^{\\text{OE}}(\\mathbf{X})=\\left \\{(i,j,l)| D(\\mathbf{x}_i,\\mathbf{x}_j) < D(\\mathbf{x}_i,\\mathbf{x}_l)\\right\\}, stores the full distance order obtained in the original space. SOE finds the embedding coordinates by solving the following optimization problem: \\min_{\\{\\hat{\\mathbf{x}}_i\\}_{i=1}^n} \\sum_{(i,j,l)\\in \\Gamma^{\\text{OE}}(\\mathbf{X})}\\text{max}^2\\left(0, \\hat{D}(\\hat{\\mathbf{x}}_i,\\hat{\\mathbf{x}}_j) +\\delta - \\hat{D}(\\hat{\\mathbf{x}}_i,\\hat{\\mathbf{x}}_l)\\right), where \\max(\\cdot,\\cdot) returns the larger one between the two input numbers, and \\delta>0 controls the embedding scale. A simple way to obtain the optimum \\{\\hat{\\mathbf{x}}_i\\}_{i=1}^n is line search gradient descent.","title":"Soft Ordinal Embedding"},{"location":"KeyKnowledge/softOrdinalEmbedding/#soft-ordinal-embedding","text":"Soft Ordianl Embedding is a typical ordinal embedding method. It constructs the embedded data by enforcing ordinal constraints derived from the original data. Given a triplet of data points (\\mathbf{x}_i, \\mathbf{x}_j, \\mathbf{x}_l) , the ordinal constraint of the triplet is settled as \\text{If } D(\\mathbf{x}_i,\\mathbf{x}_j) < D(\\mathbf{x}_i,\\mathbf{x}_l), \\text{ then } \\hat{D} (\\hat{\\mathbf{x}}_i,\\hat{\\mathbf{x}}_j) < \\hat{D} (\\hat{\\mathbf{x}}_i,\\hat{\\mathbf{x}}_l). Here D(\\cdot,\\cdot) denotes the distance between two data points in the original space, while \\hat{D}(\\cdot,\\cdot) denotes the distance in the embedded space. In the non-metric embedding setting, the value of D(\\mathbf{x}_i,\\mathbf{x}_j) is unknown but the distance order reflected by the left inequality of the above equation is known. The triplet set, as \\Gamma^{\\text{OE}}(\\mathbf{X})=\\left \\{(i,j,l)| D(\\mathbf{x}_i,\\mathbf{x}_j) < D(\\mathbf{x}_i,\\mathbf{x}_l)\\right\\}, stores the full distance order obtained in the original space. SOE finds the embedding coordinates by solving the following optimization problem: \\min_{\\{\\hat{\\mathbf{x}}_i\\}_{i=1}^n} \\sum_{(i,j,l)\\in \\Gamma^{\\text{OE}}(\\mathbf{X})}\\text{max}^2\\left(0, \\hat{D}(\\hat{\\mathbf{x}}_i,\\hat{\\mathbf{x}}_j) +\\delta - \\hat{D}(\\hat{\\mathbf{x}}_i,\\hat{\\mathbf{x}}_l)\\right), where \\max(\\cdot,\\cdot) returns the larger one between the two input numbers, and \\delta>0 controls the embedding scale. A simple way to obtain the optimum \\{\\hat{\\mathbf{x}}_i\\}_{i=1}^n is line search gradient descent.","title":"Soft Ordinal Embedding"}]}